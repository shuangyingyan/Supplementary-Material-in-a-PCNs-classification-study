import os
import re
import numpy as np
import pandas as pd
import pydicom
from glob import glob

# ============ è·¯å¾„é…ç½® ============
FEATURE_CSV_PATH = r"D:\xiaojuan\V2\radiomics_features_all_casesV2.csv"
DICOM_ROOT_DIR   = r"C:\new1\V3\images"
OUTPUT_DIR       = r"D:\xiaojuan\V2"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# æ‰¹æ¬¡æ¥æºï¼š"Auto"ï¼ˆé»˜è®¤ï¼ŒSite ä¼˜å…ˆï¼‰/"Site"/"Scanner"
BATCH_SOURCE = "Auto"
REF_BATCH = None  # ä¾‹å¦‚è®¾ä¸ºæŸä¸ªçœŸå®å­˜åœ¨çš„ç«™ç‚¹å

# ============ æ–‡æœ¬è§„èŒƒåŒ–å·¥å…· ============
def _clean_text(x):
    if x is None:
        return None
    s = str(x).strip()
    s = re.sub(r"\s+", " ", s)
    if s == "" or s.lower() in {"unknown", "none", "na", "n/a"}:
        return None
    return s

# ============ æå– DICOM å…ƒæ•°æ® ============
def extract_dicom_metadata(dicom_dir):
    rows, logs = [], []
    patients = [f for f in os.listdir(dicom_dir) if os.path.isdir(os.path.join(dicom_dir, f))]
    try:
        patients = sorted(patients, key=lambda x: int(x))
    except ValueError:
        patients = sorted(patients)

    for folder in patients:
        folder_path = os.path.join(dicom_dir, folder)
        dcm_files = glob(os.path.join(folder_path, "*.dcm")) or glob(os.path.join(folder_path, "*.DCM"))
        if not dcm_files:
            msg = f"[è­¦å‘Š] æ‚£è€… {folder} æœªæ‰¾åˆ° DICOM æ–‡ä»¶ï¼ˆ{folder_path}ï¼‰"
            print(msg); logs.append(msg)
            continue

        try:
            ds = pydicom.dcmread(dcm_files[0], stop_before_pixels=True)

            manufacturer  = _clean_text(getattr(ds, "Manufacturer", None))
            model         = _clean_text(getattr(ds, "ManufacturerModelName", None))
            serial        = _clean_text(getattr(ds, "DeviceSerialNumber", None))
            station       = _clean_text(getattr(ds, "StationName", None))

            inst_name     = _clean_text(getattr(ds, "InstitutionName", None))
            inst_dept     = _clean_text(getattr(ds, "InstitutionalDepartmentName", None))
            inst_addr     = _clean_text(getattr(ds, "InstitutionAddress", None))

            scanner = station or serial or model or "Unknown"
            site    = inst_name or inst_dept or inst_addr or "Unknown"

            # æ„é€  Batch
            if BATCH_SOURCE.lower() == "site":
                batch = site
            elif BATCH_SOURCE.lower() == "scanner":
                batch = scanner
            else:
                combo = None
                if (inst_name or inst_addr) and (manufacturer or model):
                    combo = f"{inst_name or inst_addr}|{manufacturer or ''}|{model or ''}"
                elif manufacturer or model or station:
                    combo = f"{manufacturer or ''}|{model or ''}|{station or ''}"
                batch = site if site and site != "Unknown" else (combo if combo and combo.strip("|") else scanner)

            try:
                patient_id = str(int(folder))
            except ValueError:
                patient_id = folder.strip()

            rows.append({
                "ID": patient_id,
                "Site": site or "Unknown",
                "Scanner": scanner or "Unknown",
                "Manufacturer": manufacturer or "Unknown",
                "Model": model or "Unknown",
                "StationName": station or "Unknown",
                "Batch": (batch or "Unknown")
            })
            logs.append(f"ID={patient_id} | Site={site} | Scanner={scanner} | Batch={batch}")

        except Exception as e:
            msg = f"[é”™è¯¯] è¯»å–æ‚£è€… {folder} å¤±è´¥: {e}"
            print(msg); logs.append(msg)

    log_path = os.path.join(OUTPUT_DIR, "metadata_extraction_log.txt")
    with open(log_path, "w", encoding="utf-8") as f:
        f.write("\n".join(logs))
    print(f"ğŸ“ å…ƒæ•°æ®æ—¥å¿—: {log_path}")
    return pd.DataFrame(rows)

# ============ æ”¹è¿›çš„ç‰¹å¾æ•°å€¼åŒ–å‡½æ•° ============
def coerce_features_to_numeric(df, candidate_cols, min_numeric_ratio=0.5):
    """
    å½»åº•æ”¹è¿›çš„ç‰¹å¾æ•°å€¼åŒ–å‡½æ•°
    """
    df_num = pd.DataFrame(index=df.index)
    kept, dropped = [], []
    
    print(f"\nğŸ” å¼€å§‹æ•°å€¼åŒ–è½¬æ¢ {len(candidate_cols)} ä¸ªå€™é€‰ç‰¹å¾...")
    
    for i, c in enumerate(candidate_cols):
        if i % 50 == 0:
            print(f"  å¤„ç†è¿›åº¦: {i}/{len(candidate_cols)}")
            
        s = df[c]
        
        # è·³è¿‡å…¨ä¸ºç©ºçš„åˆ—
        if s.isna().all():
            dropped.append((c, "å…¨ä¸ºç©ºå€¼"))
            continue
            
        # è®°å½•åŸå§‹æ•°æ®ç±»å‹
        orig_dtype = s.dtype
        orig_sample = s.head(3).tolist() if len(s) > 0 else []
        
        try:
            # æ–¹æ³•1: ç›´æ¥è½¬æ¢
            parsed = pd.to_numeric(s, errors='coerce')
            numeric_count = parsed.notna().sum()
            numeric_ratio = numeric_count / len(s)
            
            # å¦‚æœç›´æ¥è½¬æ¢æ•ˆæœä¸å¥½ï¼Œå°è¯•å­—ç¬¦ä¸²æ¸…ç†
            if numeric_ratio < min_numeric_ratio:
                # æ–¹æ³•2: å­—ç¬¦ä¸²æ¸…ç†åè½¬æ¢
                s_clean = s.astype(str).str.replace(',', '.', regex=False)
                s_clean = s_clean.str.replace(r'[^\d\.\-eE]', '', regex=True)
                s_clean = s_clean.replace(['', 'nan', 'None', 'null', 'NA'], np.nan)
                parsed = pd.to_numeric(s_clean, errors='coerce')
                numeric_count = parsed.notna().sum()
                numeric_ratio = numeric_count / len(s)
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¿ç•™æ¡ä»¶
            if numeric_ratio >= min_numeric_ratio and numeric_count > 0:
                df_num[c] = parsed
                kept.append(c)
                print(f"  âœ… ä¿ç•™: {c} (è½¬æ¢ç‡: {numeric_ratio:.3f}, åŸå§‹ç±»å‹: {orig_dtype}, æ ·æœ¬: {orig_sample})")
            else:
                dropped.append((c, f"æ•°å€¼æ¯”ä¾‹è¿‡ä½: {numeric_ratio:.3f}"))
                print(f"  âŒ ä¸¢å¼ƒ: {c} (è½¬æ¢ç‡: {numeric_ratio:.3f}, åŸå§‹ç±»å‹: {orig_dtype})")
                
        except Exception as e:
            dropped.append((c, f"è½¬æ¢å¼‚å¸¸: {str(e)}"))
            print(f"  âŒ å¼‚å¸¸: {c} - {str(e)}")
    
    print(f"\nğŸ“Š æ•°å€¼åŒ–ç»“æœ: ä¿ç•™ {len(kept)} ä¸ª, ä¸¢å¼ƒ {len(dropped)} ä¸ª")
    return df_num, kept, dropped

# ============ ä¸»æµç¨‹ ============
def main():
    print("ğŸ” æ­¥éª¤1: è¯»å–ç‰¹å¾ CSV ...")
    df_feat = pd.read_csv(FEATURE_CSV_PATH)

    if "ID" not in df_feat.columns:
        raise KeyError("ç‰¹å¾ CSV å¿…é¡»åŒ…å« 'ID' åˆ—")

    # æ¸…ç† CSV ä¸­å¯èƒ½çš„æ‚é¡¹åˆ—
    df_feat = df_feat.loc[:, ~df_feat.columns.duplicated()]
    drop_like = [c for c in df_feat.columns if str(c).startswith("Unnamed:")]
    if drop_like:
        df_feat = df_feat.drop(columns=drop_like)

    # è§„èŒƒ ID
    df_feat["ID"] = df_feat["ID"].astype(str).apply(lambda x: re.sub(r"\.nii(\.gz)?$", "", x.strip(), flags=re.IGNORECASE))
    print(f"âœ… åŠ è½½ç‰¹å¾ï¼š{len(df_feat)} æ ·æœ¬ï¼Œ{len(df_feat.columns)-1} ä¸ªç‰¹å¾ï¼ˆä¸å« IDï¼‰")
    
    # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
    print("\nğŸ“‹ ç‰¹å¾æ•°æ®å‰3è¡Œ:")
    print(df_feat.head(3))
    print(f"\nğŸ“‹ ç‰¹å¾åˆ—å ({len(df_feat.columns)} åˆ—):")
    for i, col in enumerate(df_feat.columns):
        print(f"  {i:3d}: {col}")

    print("ğŸ“ æ­¥éª¤2: æå– DICOM å…ƒæ•°æ® ...")
    df_meta = extract_dicom_metadata(DICOM_ROOT_DIR)
    if df_meta.empty:
        raise RuntimeError("æœªèƒ½ä» DICOM æå–å…ƒæ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–æ–‡ä»¶ã€‚")
    print(f"âœ… æå–å…ƒæ•°æ®ï¼š{len(df_meta)} ä½æ‚£è€…")

    print("ğŸ”— æ­¥éª¤3: åˆå¹¶ç‰¹å¾ä¸å…ƒæ•°æ® ...")
    df = df_feat.merge(df_meta, on="ID", how="left")

    for col in ["Site", "Scanner", "Batch"]:
        df[col] = df[col].fillna("Unknown")
        df.loc[df[col].astype(str).str.strip().eq(""), col] = "Unknown"

    merged_csv = os.path.join(OUTPUT_DIR, "radiomics_features_with_site.csv")
    df.to_csv(merged_csv, index=False)
    print(f"ğŸ’¾ å·²ä¿å­˜ï¼š{merged_csv}")

    # æ‰“å°åˆ†å¸ƒ
    print("\nğŸ“Š æ‰¹æ¬¡ï¼ˆBatchï¼‰åˆ†å¸ƒï¼š")
    print(df["Batch"].value_counts().head(20))
    print("\nğŸ“Š Site åˆ†å¸ƒï¼š")
    print(df["Site"].value_counts().head(20))
    print("\nğŸ“Š Scanner åˆ†å¸ƒï¼š")
    print(df["Scanner"].value_counts().head(20))

    # ============ æ­¥éª¤4: è¯†åˆ«ç‰¹å¾åˆ—å¹¶æ•°å€¼åŒ– ============
    meta_cols = {"ID", "Site", "Scanner", "Manufacturer", "Model", "StationName", "Batch", "Label", "Modality"}
    candidate_cols = [c for c in df.columns if c not in meta_cols]
    
    print(f"\nğŸ¯ è¯†åˆ«åˆ° {len(candidate_cols)} ä¸ªå€™é€‰ç‰¹å¾åˆ—:")
    for col in candidate_cols[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        sample_vals = df[col].dropna().head(3).tolist()
        print(f"  {col}: {sample_vals} (ç±»å‹: {df[col].dtype})")

    # ä½¿ç”¨æ”¹è¿›çš„æ•°å€¼åŒ–å‡½æ•°ï¼Œé™ä½é˜ˆå€¼ä»¥ç¡®ä¿æ›´å¤šç‰¹å¾è¢«ä¿ç•™
    df_num, feature_cols, dropped = coerce_features_to_numeric(df, candidate_cols, min_numeric_ratio=0.5)

    # è®°å½•è¢«ä¸¢å¼ƒåˆ—
    dropped_report = os.path.join(OUTPUT_DIR, "dropped_feature_columns.txt")
    with open(dropped_report, "w", encoding="utf-8") as f:
        for name, reason in dropped:
            f.write(f"{name}\t{reason}\n")

    kept_report = os.path.join(OUTPUT_DIR, "kept_feature_columns.txt")
    with open(kept_report, "w", encoding="utf-8") as f:
        for name in feature_cols:
            f.write(f"{name}\n")

    print(f"\nğŸ§¾ è¯†åˆ«åˆ°æ•°å€¼ç‰¹å¾åˆ—ï¼š{len(feature_cols)} ä¸ªï¼ˆæ¸…å•è§ {kept_report}ï¼‰")
    print(f"ğŸ§¾ è¢«ä¸¢å¼ƒå€™é€‰åˆ—ï¼š{len(dropped)} ä¸ªï¼ˆåŸå› è§ {dropped_report}ï¼‰")

    if len(feature_cols) == 0:
        print("âŒ é”™è¯¯: æœªè¯†åˆ«åˆ°ä»»ä½•æ•°å€¼ç‰¹å¾åˆ—!")
        print("å¯èƒ½çš„åŸå› :")
        print("1. ç‰¹å¾CSVæ–‡ä»¶æ ¼å¼é—®é¢˜")
        print("2. ç‰¹å¾å€¼åŒ…å«å¤§é‡éæ•°å€¼å­—ç¬¦") 
        print("3. ç‰¹å¾åˆ—åè¯†åˆ«é”™è¯¯")
        print("4. æ•°æ®ç¡®å®å…¨ä¸ºç©ºå€¼")
        
        # è¯¦ç»†è¯Šæ–­
        print("\nğŸ” è¯¦ç»†è¯Šæ–­ä¿¡æ¯:")
        for col in candidate_cols[:5]:  # æ£€æŸ¥å‰5ä¸ªå€™é€‰åˆ—
            sample_data = df[col].dropna().head(5).tolist()
            print(f"åˆ— '{col}': æ ·æœ¬å€¼ = {sample_data}, ç±»å‹ = {df[col].dtype}")
        
        raise RuntimeError("æœªè¯†åˆ«åˆ°ä»»ä½•æ•°å€¼ç‰¹å¾åˆ—ã€‚è¯·æ£€æŸ¥ dropped_feature_columns.txt ä»¥å®šä½åŸå› ã€‚")

    # ============ æ­¥éª¤5: ç¼ºå¤±å€¼å¤„ç† ============
    print(f"\nğŸ”§ æ­¥éª¤5: ç¼ºå¤±å€¼å¤„ç†...")
    X = df_num[feature_cols].to_numpy(dtype=float)
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    nan_count = np.isnan(X).sum()
    inf_count = np.isinf(X).sum()
    print(f"  ç¼ºå¤±å€¼æ•°é‡: {nan_count}")
    print(f"  æ— ç©·å€¼æ•°é‡: {inf_count}")
    
    # å¤„ç† NaN/inf
    if not np.isfinite(X).all():
        X[~np.isfinite(X)] = np.nan
        print("  âš ï¸ æ£€æµ‹åˆ°éæœ‰é™å€¼ï¼Œå·²è½¬æ¢ä¸ºNaN")
    
    if np.isnan(X).any():
        col_mean = np.nanmean(X, axis=0)
        # å¯¹äºå…¨ä¸ºNaNçš„åˆ—ï¼Œç”¨0å¡«å……
        col_mean[np.isnan(col_mean)] = 0
        inds = np.where(np.isnan(X))
        X[inds] = np.take(col_mean, inds[1])
        print(f"  âœ… å·²ç”¨åˆ—å‡å€¼å¡«å…… {np.isnan(X).sum()} ä¸ªç¼ºå¤±å€¼")

    # ============ æ­¥éª¤6: ComBatï¼ˆneuroHarmonizeï¼‰ ============
    print(f"\nğŸ”§ æ­¥éª¤6: ComBat æ‰¹æ¬¡æ ¡æ­£...")
    batch_series = df["Batch"].astype(str).fillna("Unknown")
    n_batches = batch_series.nunique(dropna=False)
    
    if n_batches < 2:
        print("â„¹ï¸ ä»…æ£€æµ‹åˆ° 1 ä¸ªæ‰¹æ¬¡ï¼Œè·³è¿‡ ComBatï¼Œç›´æ¥ä¿å­˜ç‰¹å¾ã€‚")
        X_adj = X.copy()
    else:
        print(f"  æ£€æµ‹åˆ° {n_batches} ä¸ªæ‰¹æ¬¡ï¼Œè¿›è¡Œ ComBat æ ¡æ­£...")
        # ç¡®ä¿å‚è€ƒæ‰¹æ¬¡æœ‰æ•ˆ
        ref = REF_BATCH if (REF_BATCH is not None and REF_BATCH in set(batch_series)) else None
        if REF_BATCH is not None and ref is None:
            print(f"  âš ï¸ æŒ‡å®šçš„ REF_BATCH='{REF_BATCH}' ä¸åœ¨æ•°æ®ä¸­ï¼Œå·²è‡ªåŠ¨æ”¹ä¸º Noneã€‚")

        covars = pd.DataFrame({"batch": batch_series})
        try:
            from neuroHarmonize import harmonizationLearn
            model, X_adj = harmonizationLearn(X, covars, ref_batch=ref, eb=True)
            print("  âœ… ComBat æ ¡æ­£å®Œæˆ")
        except Exception as e:
            print(f"  âŒ ComBat æ ¡æ­£å¤±è´¥: {e}")
            print("  âš ï¸ ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œåç»­å¤„ç†")
            X_adj = X.copy()

    # ============ æ­¥éª¤7: ç»“æœè½ç›˜ ============
    print(f"\nğŸ’¾ æ­¥éª¤7: ä¿å­˜ç»“æœ...")
    out_cols_first = [c for c in ["ID", "Site", "Scanner", "Batch"] if c in df.columns]
    df_out = pd.concat(
        [df[out_cols_first].reset_index(drop=True),
         pd.DataFrame(X_adj, columns=feature_cols)],
        axis=1
    )
    
    # éªŒè¯è¾“å‡ºæ•°æ®
    print(f"  è¾“å‡ºæ•°æ®å½¢çŠ¶: {df_out.shape}")
    print(f"  ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)}")
    print(f"  å‰3ä¸ªç‰¹å¾åˆ—æ ·æœ¬å€¼:")
    for col in feature_cols[:3]:
        sample_vals = df_out[col].head(3).tolist()
        print(f"    {col}: {sample_vals}")

    out_csv = os.path.join(OUTPUT_DIR, "radiomics_features_combat_harmonized.csv")
    df_out.to_csv(out_csv, index=False)
    print(f"âœ… å®Œæˆï¼å·²ä¿å­˜ï¼š{out_csv}")
    
    # æœ€ç»ˆéªŒè¯
    final_check = pd.read_csv(out_csv)
    feature_data_present = final_check[feature_cols].notna().any().any()
    if feature_data_present:
        print("ğŸ‰ éªŒè¯é€šè¿‡: è¾“å‡ºæ–‡ä»¶åŒ…å«æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®!")
    else:
        print("âŒ è­¦å‘Š: è¾“å‡ºæ–‡ä»¶ä¸­çš„ç‰¹å¾æ•°æ®å¯èƒ½ä»ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼!")

if __name__ == "__main__":
    main()